%\usepackage{algorithm}
%\usepackage{algorithmic}

\section{Implementation in FreeFem++}
First we have to define problems we need to solve at each iteration. 
We have 
\begin{align}
&\left\lbrace 
	\begin{matrix}\label{eq1}
	& \Delta \phi &= G \\
	& \left. \phi\right|_{\partial\Omega} &= 0
	\end{matrix}
\right.\\
&\left\lbrace 
	\begin{matrix}\label{eq2}
	& \Delta \lambda &\hspace{-40pt}= 0 \\
	& \left. \lambda\right|_{\partial\Omega} &= g(s)-\frac{\partial \phi}{\partial n}
	\end{matrix}
\right.
\end{align}
where g is an approximate measurement or prescription of $\frac{\partial \phi_0}{\partial n}$. 

We define two problems named \verb|a| and \verb|b| that solves respectively problems (\ref{eq1}) and (\ref{eq2}).
Moreover, we solve a problem called \verb|initial| for $\Delta \phi_0 = G(x,x_0,y,y_0)$. This is purely artificial as we are not given $g =\frac{\partial \phi_0}{\partial n}$ per measurement/prescription.
\lstinputlisting[language=C++]{../problems.edp}

We use a generic gradient descent algorithm to find the source position.\\
\begin{algorithm}
\caption{Gradient algorithm}
\begin{algorithmic} 
\WHILE{$||\nabla f(x_k)|| \geqslant \epsilon$}
\STATE calculation of $\nabla f(x_k)$.
\STATE calculation of $\alpha_k$
\STATE $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

We know that $-\frac{\partial I}{\partial x_0} = \int_{\Omega}\lambda \frac{\partial G}{\partial x_0} dx$ and $-\frac{\partial I}{\partial y_0} = \int_{\Omega}\lambda \frac{\partial G}{\partial y_0} dx$; it remains to compute these expressions.

 $\nabla G(x_0,y_0)$ we get straight-forwardly through finite differences. For $\lambda$ we compute problem \verb|b| for which we need the solution $\phi$ to problem \verb|a| first.

Then we choose the coefficient $\alpha_k$ according to the magnitude of the gradient norm. Since its size may vary depending on the optimal position $(x_0,y_0)$, we look at the ratio between initial norm \verb|refnorm| and current \verb|norm| (both are squared). If the norm is relatively large, we use an effective step size of $\sim 0.1$ ($\alpha = 0.1\:\sqrt{\verb|norm|}^{-1}$), while in the regime close to optimum we choose $\alpha=1$.

\lstinputlisting[language=C++]{../gradient.edp}

